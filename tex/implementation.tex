\chapter{Implementation}

This chapter describes the design and implementation of a new service adding support for running workflows on clouds, with the specific target infrastructure being AWS EC2 machines. The deployment, configuration and lifecycle management of the underlying nodes executing the workflow should be transparent to the user, who is only required to provide his credentials to an AWS account.

In accordance with the considerations presented in Chapter \ref{DesignChapter} about the architecture and implementation details of both OpenMOLE and GridScale, we take a layered approach in adding support for a new cloud environment.

The first part of the chapter discusses the approach taken to bootstrap and configure a cluster consisting of EC2 machines. The cluster needs to be coordinated by a scheduler that can take job submissions and distribute them for execution.

We then proceed to analyse the integration of the cloud service with the overall OpenMOLE application. Here we treat issues such as triggering the teardown of the cluster depending on the batch submission activity and the inherent differences between the new cloud environment and the already existing types of environments.

Although we focus on deployment on AWS, the same high-level technique can easily be extended to support other cloud providers, so at the end of the chapter we briefly discuss the details of adding support for the Google Compute Engine infrastructure.

Throughout the chapter, we motivate choices made along the development process and detail problems or advantages of alternative approaches. Since the main challenge of the implementation is reliably wiring together different libraries, command line tools and cloud services, we favour the use of diagrams and code listings to show the interaction between core system components.

\section{GridScale AWS Module}

As explained in Section \ref{GridScaleSection}, access to each target environment in GridScale is implemented via a module. For the purpose of implementing an AWS module, we need to provide each of the four components: a \textit{Job Description}, a \textit{Job Service}, a \textit{Storage} interface and an \textit{Authentication} method.

\subsection{Design Motivation} \label{DesignMotivationSection}

The initial idea was to simply implement an AWS specific version of each component. This is though not necessary, as we can reuse some of the already existing components from other modules and GridScale in fact encourages this approach through its modular packaging using OSGi bundles.

In particular, GridScale already supports cluster environments managed by specific schedulers. By building a cluster from a set of machines provisioned from AWS and configuring to emulate the required behaviour, we are able to partially delegate the job management work to an already functional cluster job service. This means that, in contrast with a regular cluster service that acts as a DAO\footnote{Data Access Object} for an existing infrastructure, we also need to provide interface methods through which users of the library can manage the lifecycle of the deployed cluster. We argue that this is a sane choice from an engineering perspective, since we are already familiar with reliably supported schedulers like SGE and Slurm.

The type of the job description depends on the job service, since it performs a translation to a script that can be sent to the scheduler. Therefore, forwarding the submission and monitoring methods to a cluster service as discussed above implies that we need to directly use the description corresponding to the job service.

As long as we can deploy a cluster with a filesystem shared across the network, the \verb|SSHStorage| trait allows accessing the same data volume for the master and slave nodes. EC2 instances can be backed up by either EBS volumes or internal storage, but the solution of installing NFS on the cluster is agnostic to the underlying storage choice and enables using a wide range of instance types.

Authentication is indeed based on the AWS credentials stored locally by the user and requires a new model. However, the standard authentication implementation relying on SSH keys can still be reused as a lower layer of this new implementation in order to access the master node that controls the job submission queue.

External applications make use of the AWS module by instantiating an \verb|AWSJobService|, which publicly exposes the following methods:

\begin{itemize}
	\item \verb|start| launches and configures a job submission cluster to be used by the service.
	\item \verb|close| completely tears down the job cluster.
	\item \verb|submit|, \verb|cancel|, \verb|state| and \verb|purge| fulfil their regular functions within the service.
\end{itemize}

Listing \ref{AWSJobService} shows an example usage of the service. The service takes as parameters the region the job should run in, as well as the number of EC2 instances that the service should create under the \verb|clusterSize| parameter. The other authentication-related parameters are discussed in Section \ref{ClusterDeploymentSection}.

\begin{listing}[h]
	\centering
	\begin{minipage}{11.8cm}
		\begin{minted}[frame=single,framesep=2mm,baselinestretch=1.11,fontsize=\small,linenos]{scala}
val awsService = AWSJobService(AWSJobService.Config(
    region = "eu-west-1",
    awsUserName = "adrian",
    awsCredentialsPath = "/Users/adrian/.aws/credentials.csv",
    awsUserId = "434676269080",
    awsKeypairName = "gridscale",
    privateKeyPath = "/Users/adrian/.ssh/id_rsa",
    clusterSize = 1))

awsService.start()

val description = new AWSJobDescription {
  def executable = "/bin/sleep"
  def arguments = "5"
  def workDirectory = aws.home
}

val job = awsService.submit(description)
while (awsService.state(job) != Done) {
  Thread.sleep(WAIT_TIME)
}

awsService.purge(job)
awsService.close()
		\end{minted}
	\end{minipage}
	\caption{Submitting a job to the cloud using the AWS module.}
	\label{AWSJobService}
\end{listing}

\vspace{-5mm}
\subsection{Cluster Deployment} \label{ClusterDeploymentSection}

The design of the module presented above outlines the requirement for a system that can create clusters of AWS EC2 machines and configure them with a job scheduler and a shared filesystem across all nodes.

\textbf{Tool Choice}

During the investigations in Section \ref{ClusterDeploymentSection}, we found that most cost-efficient tools and frameworks for creating the cluster are StarCluster, Elasticluster and Jclouds. The main reason for excluding Mesosphere DC/OS was that it is generally heavyweight and requires running more expensive machines to power the cluster. CfnCluster was dismissed for being too reliant on the AWS software stack, which incurs various extra costs for operations that can easily be performed without Amazon resources.

Jclouds allows mounting various cloud storage devices, but it did not fit our use case since it does not have built-in support for NFS installations. Although Elasticluster can perform all the required tasks, we opted for StarCluster as the cloud deployment tool, since it also provides a load balancer that can be used to optimize the cost or run time of a workflow.

\vspace{3mm}
\textbf{Coordinator Node}
\vspace{1mm}

The choice for StarCluster raises the problem that it is only offered as a command line tool and it needs to be on the machine it is run on. Although most cluster orchestration tools require manual installation and interaction, forcing the user of the library to install an external is both unreasonable and impractical from GridScale's perspective. 

An initial idea could have been running a Docker container with StarCluster, but we can, once again, not assume the presence of a Docker engine on the user's machine.

The solution we picked was to create an EC2 instance with StarCluster already installed. We call this particular instance the cluster \textit{coordinator} or \textit{orchestrator}, since it controls the whole cluster activity by executing the StarCluster commands to operate it. 

The coordinator is launched using Jclouds within the \verb|start| method of the \verb|AWSJobService| created on the local machine. Its configuration is based on a prebuilt AMI\footnote{Public AMI ID: ami-b7d8cedd} with an installation of StarCluster and maintained by the developers of GridScale. After the coordinator is in a running state and its public IP address has been established, the \verb|AWSJobService| can start sending commands to it via an SSH channel. This represents step 1 in Figure \ref{CoordinatorSetup}.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.93\linewidth]{CoordinatorSetup.png}
	\caption{Creating a new cluster using a proxy coordinator node and StarCluster.}
	\label{CoordinatorSetup}
\end{figure}

In order to start the coordinator node, Jclouds needs to authenticate the user. Amazon encourages using IAM\footnote{Identity Access Management} roles for authentication. This means that the user does not need to provide the root account password, but can instead create a user associated with GridScale, which is granted access to manage EC2 instances. Jclouds then uses the access key ID and secret key corresponding to the user role to manage machines via the API. The file containing the keys can be downloaded by the user from the Security Credentials page in the AWS Console \cite{AWSCredentials}. The path to the CSV credentials file must be passed as the \verb|awsCredentialsPath| parameter when creating a new \verb|AWSJobService|.

Apart from using the IAM credentials to make EC2 API calls, we are also opening SSH channels to deliver commands to the coordinator. To do this we need to register the SSH key we are using to initiate the connection to the coordinator on AWS. This is achieved by importing the public key on AWS and giving it a name. The \verb|awsKeypairName| associated with the private key is then always mentioned when openning an SSH connection.

Although it might seem that introducing a proxy node for passing StarCluster commands is inefficient, the actual overhead is minimal. The reason for the limited impact is that StarCluster is only explicitly invoked during the initialisation and destruction of the cluster. All job scheduling interactions with the job submission controller bypass the coordinator, since the job service can obtain the address of the master node and send it jobs directly. The coordinator does indeed incur a cost for being provisionned for as long as the cluster runs, but this is insignificant since we only need one of the cheapest instance types to run it. Further details are discussed in Section \ref{CoordinatorOverheadSection}.

\vspace{3mm}
\textbf{StarCluster Configuration}
\vspace{1mm}

StarCluster relies on a configuration file instead of command line parameters to specify the characteristics of clusters it is creating. The file must be located on the machine where StarCluster is being run, so we need to ensure that it is present on the coordinator node. Since the configuration parameters depend on the preferences of the user, the file can not be statically embedded in the AMI that the coordinator is constructed from and needs to be generated dynamically. Listing \ref{StarClusterConfig} shows an example StarCluster configuration file constructed on-the-fly.

\begin{listing}[h]
	\centering
	\begin{minipage}{9.4cm}
		\begin{minted}[frame=single,framesep=2mm,baselinestretch=1.11,fontsize=\small,linenos]{text}
[global]
DEFAULT_TEMPLATE = jobcluster
ENABLE_EXPERIMENTAL = True

[aws info]
AWS_REGION_NAME = eu-west-1
AWS_REGION_HOST = ec2.eu-west-1.amazonaws.com
AWS_ACCESS_KEY_ID = <access-key-id>
AWS_SECRET_ACCESS_KEY = <secret-access-key>
AWS_USER_ID = <user-id>

[key starcluster-d4c9b13a]
KEY_LOCATION = .starcluster/starcluster-d4c9b13a

[cluster jobcluster]
KEYNAME = starcluster-d4c9b13a
CLUSTER_SIZE = 1
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-044abf73
MASTER_INSTANCE_TYPE = m3.medium
NODE_INSTANCE_TYPE = m1.small
		\end{minted}
	\end{minipage}
	\caption{StarCluster configuration file.}
	\label{StarClusterConfig}
\end{listing}


The authentication method used for bringing up EC2 instances is the same as the one used by Jclouds and StarCluster also needs an SSH keypair in order to set up SSH access to the master node and password-less SSH within the nodes in the cluster. Therefore, as step 2 in Figure \ref{CoordinatorSetup}, we first create a new private key, saved in this case as \verb|starcluster-d4c9b13a|, and import it into AWS.

Step 3 is creating the \verb|config| file on the coordinator node from the parameters passed to the \verb|AWSJobService|. After the job service triggers the cluster initialization at step 4, the coordinator performs the launch using the \verb|starcluster start| command at step 5.

The actions above leave the system in a state where the AWS cluster is initialized and configured with the SGE scheduler, so we can start submitting jobs to the master node, which acts as a submission controller. For the purpose of job management, the job service should now ignore the coordinator and communicate directly with the master of the cluster. Figure \ref{MasterConnection} illustrates how this is achieved.

After obtaining the address of the master, the local machine needs to establish an SSH connection with it. Instead of creating a new set of SSH keys, we chose to reuse the keypair used by the coordinator to communicate with the cluster and transfer the private key locally under a temporary \verb|~/.gridscale| directory. This keypair is already set up for connections to all nodes in the cluster and helps avoid the complexity of managing even more keys.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.96\linewidth]{MasterConnection.png}
	\caption{Connecting the local machine with the master node via SSH and distributing jobs.}
	\label{MasterConnection}
\end{figure}

\vspace{2mm}
\textbf{SGE Delegation}
\vspace{1mm}

StarCluster deploys SGE as its scheduler and, as discussed in Section \ref{DesignMotivationSection}, we are able to delegate all job submission methods of the \verb|AWSJobService| to an \verb|SGEJobService|. This is possible thanks to full compatibility between the interfaces of different GridScale modules. A side effect is that \verb|AWSJobDescription| is just a subtype of \verb|SGEJobDescription|, as we are in fact submitting SGE jobs. The \verb|AWSJobService| could have used \verb|SGEJobDescription|s, but we chose to hide this module dependency from the user of the library.

 Tasks are specified in the same way as for plain SGE and are eventually translated to SGE jobs. Steps 4 and 5 in Figure \ref{MasterConnection} show the normal job submission mechanism after the setup phase is finished. Once a queue of jobs is populated, the work is distributed by the scheduling system to all nodes in the cluster.\\

\vspace{3mm}
\textbf{Cluster Lifecycle}
\vspace{1mm}

The coordinator node and the cluster are tightly coupled in our design because the cluster can not be altered without the coordinator, which, in turn, is not relevant on its own. Therefore it makes sense for the lifecycle of the two components to be managed together, so the \verb|close| method of \verb|AWSJobService| shuts down the cluster, destroys the orchestrator and kills Jclouds communication channels to AWS.

Even though \verb|start| and \verb|close| could have been implicitly called respectively when an instance of the job service is created and when all jobs are finished, keeping them in the public API allows the user of a library more flexibility towards reusing the service for more sessions without having to repeatedly wait for the initialization steps.

\subsection{Load Balancing}

We use the built-in StarCluster load balancer to decrease or increase the number of nodes in the cluster between limits specified by the user. We start the daemon that performs job monitoring right after launching the cluster and keep the standard 60 seconds polling interval. 

If the user does not specify a \verb|minimumClusterSize| or \verb|maximumClusterSize| for elastic scaling purposes when creating the job service, then we allow the cluster to shrink or extend in size between 1 node and its initial size. We argue that this is the cautious choice, since it can potentially only reduce costs and not incur any unexpected ones.

Another option we expose is to automatically killing the cluster (by setting the \verb|autoClose| flag) if it is stays completely idle for a certain period after all jobs have been terminated. Some consumers of the library may prefer this as the default option rather than having to explicitly estimate when work is done and call \verb|close|.

To obtain a highly elastic cluster, users can also reduce the 3 minute lookback window via the \verb|lookbackWindow| parameter, which represents the time after which an idle node is considered for elimination. However, note that nodes will never be eliminated unless they have already been running for at least 45 minutes of the hour they have been rented for. Due to Amazon's hourly rental policy, an overly eager resource cutting approach would be non-optimal, since we would be giving away power that has already been paid for and might be employed in case of a sudden spike in job submissions.

\subsection{Spot Instances}

We support running a spot cluster by leveraging StarCluster to bid for the desired type of worker instances. An important consideration is that only slave nodes in the cluster are allowed to be spots, since we can easily deal with their downscaling by requeuing failed jobs. The master is always a dedicated on-demand instance, because having it suddenly killed would require restarting the whole cluster.

GridScale automates the spot bidding decision by using a simple heuristic based on the current bids for the instance and the average for the past month. We assume that the correct price will always tend towards the average, so we choose a bid depending on the relation between the current price and the average. 

If the current market price is lower than the average, then we are ahead of the bidders that will soon lose their machines as prices will rise. In the opposite scenario, we know that the equilibrium price is lower than the current one, so we can afford to bid less than the market price while still remaining confident to be competitive.

If the current market price is lower than the average, then we are safe to use the instance for at least a couple of hours by simply bidding the average value, since spot prices fluctuate at a relatively steady rate. In the opposite scenario, we bid the market price plus an additional 10\% to account for unexpected change, but we assume that the price will tend towards the average.

Overall, the strategy is rather conservative. We believe that no aggressive bids are needed to reduce renting costs, because prices for spot instances are already approximately 6 times lower than for regular on-demand ones as of June 2016 in US East, as shown in Table \ref{SpotPricing}. Section \ref{OpenMOLEAWSSection} also details how OpenMOLE can gracefully deal with spot instances being lost, so complicated strategy to optimise bidding only has diminishing returns.

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{Price (\$ per hour)}} \\ \cline{2-3} 
\multicolumn{1}{c|}{\textbf{Instance type}} & \multicolumn{1}{c|}{\textbf{Spot}} & \multicolumn{1}{c|}{\textbf{On-Demand}} \\ \hline
\multicolumn{1}{|c|}{c3.large} & \multicolumn{1}{c|}{0.0177} & \multicolumn{1}{c|}{0.105} \\ \hline
\multicolumn{1}{|c|}{m3.medium} & \multicolumn{1}{c|}{0.0108} & \multicolumn{1}{c|}{0.067} \\ \hline
\multicolumn{1}{|c|}{m3.xlarge} & \multicolumn{1}{c|}{0.037} & \multicolumn{1}{c|}{0.266} \\ \hline
\multicolumn{1}{|c|}{m3.2xlarge} & \multicolumn{1}{c|}{0.0842} & \multicolumn{1}{c|}{0.532} \\ \hline
\end{tabular}
\caption{Differences in prices between spot and on-demand instances \cite{AWSPricing}.}
\label{SpotPricing}
\end{table}

\section{OpenMOLE AWS Environment} \label{OpenMOLEAWSSection}

The \verb|AWSEnvironment| in OpenMOLE adapts the GridScale \verb|AWSJobService| to the requirements of OpenMOLE and decorates it with extra batch submission and failure recovery capabilities. 

Since user credentials and preferences are passed directly to the GridScale module, the environment's constructor parameters are, in the simplest case when default instance types are used, identical to those of the job service. Listing \ref{AWSEnvironment} shows how we modify the simple workflow from Listing \ref{SimpleWorkflow} to run on an AWS cluster instead of the local machine.

\begin{listing}[h]
	\centering
	\begin{minipage}{11.4cm}
		\begin{minted}[frame=single,framesep=2mm,baselinestretch=1.15,fontsize=\small,linenos]{scala}
val i = Val[Int]
val res = Val[Int]

val exploration = ExplorationTask(i in (1 to 100 by 1))

val model =
  ScalaTask("val res = i * i") set (
    inputs += i,
    outputs += (i, res)
  )

val env = AWSEnvironment(
  region = "eu-west-1",
  awsUserName = "adrian",
  awsUserId = "434676269080",
  awsKeypairName = "gridscale",
  awsCredentialsPath = "/Users/adrian/.aws/credentials.csv",
  privateKeyPath = "/Users/adrian/.ssh/id_rsa",
  clusterSize = 8
)
  
exploration -< (model on env hook ToStringHook())
		\end{minted}
	\end{minipage}
	\caption{Workflow running on an AWS cluster.}
	\label{AWSEnvironment}
\end{listing}

An important feature of the cluster built using GridScale is that it provides a network shared \verb|/home| directory that is mounted on every node. This facilitates the runtime distribution in OpenMOLE, since it only needs to be copied remotely once, after which every machine can use it to run tasks in their packaged context. Section \ref{FutureWorkSection} discusses a future improvement that will allow us to avoid transferring the runtime remotely for every cluster instantiation.

\subsection{Job Service Lifecycle}

The GridScale \verb|AWSJobService| was designed specifically to allow for its lifecycle to be managed externally. In the OpenMOLE integration, we take advantage of this feature and turn off the entire environment based on events occurring in the message queue.

Specifically, we do not terminate the job service immediately after all the initial jobs have been consumed, but instead track all types of events occurring in the \verb|JobManager| allocated to the environment and only kill the cluster once all potentially failed jobs have been successfully rerun, results have been copied and assembled back on the user's machine.

\subsection{Spot Clusters}

OpenMOLE's mechanism for job submission and dealing with unexpected failures of execution nodes plays well with the concept of spot clusters and automatic scaling of the number of worker nodes.

While adding or removing nodes based on the load in the cluster does not impact the runs of any jobs because only machines that have already been idle long enough are removed, tasks running on spot instances can be unexpectedly killed when the market price of the instance rises over the current bid. This case is dealt with by continuously polling for job results and resubmitting the job if the target machine is unreachable or the job has been cancelled for external reasons. The new submission is automatically picked up by the message queue and directed to one of the healthy nodes.

\subsection{Resource Mapping}

Although the default approach followed by OpenMOLE for running workflows is to try providing sane defaults for users without deep technical expertise, one of parameters that can be specified when constructing an environment is \verb|OpenMOLEMemory|, which indicates the amount of memory that the workflow is expected to need.

For the \verb|AWSEnvironment|, resources are allocated in terms of the number and types of instances chosen to form the underlying cluster, so we allow users unwilling to rely on the default \verb|m3.medium| master and \verb|m1.small| slave nodes to specify their own preferences for the cluster machines.

Additionally, to allow seamless transition from workflows that were previously being specified using the memory requirements to cloud-based executions, we introduce a \textit{resource mapper} that translates the memory specification to a set of instances covering the desired configuration.

\section{Other Cloud Platforms}

Although the main focus of the implementation was to provide a fully functional system for running OpenMOLE experiments on AWS, the technique we used is not tied to a single cloud provider. Command line tools like CfnCluster, DC/OS and Elasticluster are similar with StarCluster in terms of interfaces and capabilities, so the cluster configuration layer can easily be replaced.

Additionally, most of the behaviour and structure of the AWS GridScale module and OpenMOLE environment can be extracted to interfaces for cloud services, especially since cloud providers themselves offer similar functionality. For instance, the \verb|AWSJobService| can be split in multiple platform agnostic components that can be combined in a generic \verb|CloudJobService|:

\begin{itemize}
	\item The coordinator node is already created using Jclouds, so it can be deployed to many commercial clouds.
	\item All the mechanisms for transferring the OpenMOLE runtime remotely, packaging jobs and configuring SSH connections to nodes in the cluster only require access to a shell, so they do not depend on where resources are provisioned from.
	\item The job submission controller can delegate responsibility for translating and monitoring jobs to existing implementations such as the \verb|SGEJobService|, the \verb|SLURMJobService| or others based on the requirements of the underlying cluster.
	\item OpenMOLE cloud environments are virtually identical, since resource access in encapsulated in the GridScale modules with matching interfaces. Once multiple providers are supported, we only need a single cloud environment that instantiates different job services.
\end{itemize}

\vspace{-1mm}
\subsection{GCE Support}

Taking Google Compute Engine as an example, we outline the main aspects that need to be considered when adding support for a new cloud platform. We do not yet provide a full working implementation for GCE, but we analyse the main differences and components that need to be changed in comparison with AWS.

The \verb|GCEJobService| acts as the interface of the GridScale GCE module and it needs to provide the same public methods as the \verb|AWSJobService|. The implementation details of starting and stopping the service depend on the tool chosen to manage the underlying cluster, while the \verb|submit|, \verb|cancel|, \verb|state| and \verb|purge| calls can be forwarded to the scheduler controlling the cluster.

Both Elasticluster and Mesosphere DC/OS provide primitives for creating and managing a GCE cluster, but we dismiss DC/OS due to the more elevated costs incurred by the usage of more powerful instances. On the other hand, Elasticluster's operating paradigm makes it very similar to StarCluster, with a few minor differences pointed out in Section \ref{ElasticlusterBackground}. 

Since we want to preserve the assumption of no configuration being necessary on behalf of the user and Elasticluster is a command line tool, it needs to be run by a remote coordinator node. The proxy node is started using the Jclouds API and SSH pairing between the local machine and the coordinator is performed by importing an SSH keypair associated with a service accounts. These serve a similar purpose as Amazon IAM roles and essentially regulate programmatic access to resources associated with the account.

The Elasticluster configuration file is based on the interplay between different Ansible playbooks and also needs to be generated dynamically. The example file shown in Listing \ref{ElastiClusterConfig} exposes similarities with StarCluster. It begins by specifying the details of the desired cloud provider, login details and Ansible playbook that provides a Slurm installation and an NFS-shared storage volume. Line 19 bring the components together and assembles a cluster with a single master node acting as an SSH entry point and two worker nodes.

\begin{listing}[h]
	\centering
	\begin{minipage}{8cm}
		\begin{minted}[frame=single,framesep=2mm,baselinestretch=1.11,fontsize=\small,linenos]{text}
[cloud / google]
provider = google
gce_client_id = <client-id>
gce_client_secret = <client-secret>
gce_project_id = <project-id>
zone = europe-west1-b	

[login / google]
image_user = <google-username>
user_key_name = elasticluster
user_key_private = ~/.ssh/id_rsa
user_key_public = ~/.ssh/id_rsa.pub

[setup / ansible-slurm]
provider = ansible
frontend_groups = slurm_master
compute_groups = slurm_clients

[cluster / gridscale-cluster]
cloud = google
login = google
setup_provider = ansible-slurm
image_id = ubuntu-1404-trusty-v20160509a
flavor = f1-micro
frontend_nodes = 1
compute_nodes = 2
ssh_to = frontend
		\end{minted}
	\end{minipage}
	\caption{Elasticluster configuration file.}
	\label{ElastiClusterConfig}
\end{listing}

After the cluster is created, the job submission workload can be forwarded to a \verb|SLURMJobService|. A caveat is that Elasticluster does not provide a built-in way of expanding or shrinking the cluster depending on the load, but this can be implemented using the existing calls for adding and removing individual nodes.

One advantage of running clusters on GCE is that instance initialisation times tend to be lower compared to AWS, resulting in a faster startup of the the whole system. GCE does not have spot instances that vary in renting costs depending on demand. Instead, it introduces the concept of preemptible instances \cite{Preemptible}, which are even 70\% cheaper than normal ones can be interrupted under specific conditions.

A \verb|GCEEnvironment| in OpenMOLE would wrap the job service and is not even necessary in the case of factoring out a generic \verb|CloudEnvinronment| that receives the required job service as a parameter. However, the resource mapping feature needs to be overriden in order to account for Google-specific instance types and prices.