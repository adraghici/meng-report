\chapter{Implementation}

This chapter describes the design and implementation of a new service adding support for running workflows on clouds, with the specific target infrastructure being AWS EC2 machines. The deployment, configuration and lifecycle management of the underlying nodes executing the workflow should be transparent to the user, who is only required to provide his credentials to an AWS account.

In accordance with the considerations presented in Chapter \ref{DesignChapter} about the architecture and implementation details of both OpenMOLE and GridScale, we take a layered approach in adding support for a new cloud environment.

The first part of the chapter discusses the approach taken to bootstrap and configure a cluster consisting of EC2 machines. The cluster needs to be coordinated by a scheduler that can take job submissions and distribute them for execution.

We then proceed to analyse the integration of the cloud service with the overall OpenMOLE application. Here we treat issues such as triggering the teardown of the cluster depending on the batch submission activity and the inherent differences between the new cloud environment and the already existing types of environments.

Although we focus on deployment on AWS, the same high-level technique can easily be extended to support other cloud providers, so at the end of the chapter we briefly discuss the details of adding support for the Google Compute Engine infrastructure.

Throughout the chapter, we motivate choices made along the development process and detail problems or advantages of alternative approaches. Since the main challenge of the implementation is reliably wiring together different libraries, command line tools and cloud services, we favour the use of diagrams and code listings to show the interaction between core system components.

\section{GridScale AWS Module}

As explained in Section \ref{GridScaleSection}, access to each target environment in GridScale is implemented via a module. For the purpose of implementing an AWS module, we need to provide each of the four components: a \textit{Job Description}, a \textit{Job Service}, a \textit{Storage} interface and an \textit{Authentication} method.

\subsection{Design Motivation} \label{DesignMotivationSection}

The initial idea was to simply implement an AWS specific version of each component. This is though not necessary, as we can reuse some of the already existing components from other modules and GridScale in fact encourages this approach through its modular packaging using OSGi bundles.

In particular, GridScale already supports cluster environments managed by specific schedulers. By building a cluster from a set of machines provisioned from AWS and configuring to emulate the required behaviour, we are able to partially delegate the job management work to an already functional cluster job service. This means that, in contrast with a regular cluster service that acts as a DAO\footnote{Data Access Object} for an existing infrastructure, we also need to provide interface methods through which users of the library can manage the lifecycle of the deployed cluster. We argue that this is a sane choice from an engineering perspective, since we are already familiar with reliably supported schedulers like SGE and SLURM.

The type of the job description depends on the job service, since it performs a translation to a script that can be sent to the scheduler. Therefore, forwarding the submission and monitoring methods to a cluster service as discussed above implies that we need to directly use the description corresponding to the job service.

As long as we can deploy a cluster with a filesystem shared across the network, the \verb|SSHStorage| trait allows accessing the same data volume for the master and slave nodes. EC2 instances can be backed up by either EBS volumes or internal storage, but the solution of installing NFS on the cluster is agnostic to the underlying storage choice and enables using a wide range of instance types.

Authentication is indeed based on the AWS credentials stored locally by the user and requires a new model. However, the standard authentication implementation relying on SSH keys can still be reused as a lower layer of this new implementation in order to access the master node that controls the job submission queue.

External applications make use of the AWS module by instantiating an \verb|AWSJobService|, which publicly exposes the following methods:

\begin{itemize}
	\item \verb|start| launches and configures a job submission cluster to be used by the service.
	\item \verb|close| completely tears down the job cluster.
	\item \verb|submit|, \verb|cancel|, \verb|state| and \verb|purge| fulfil their regular functions within the service.
\end{itemize}

Listing \ref{AWSJobService} shows an example usage of the service. The service takes as parameters the region the job should run in, as well as the number of EC2 instances that the service should create under the \verb|clusterSize| parameter. The other authentication-related parameters are discussed in Section \ref{ClusterDeploymentSection}.

\begin{listing}[h]
	\centering
	\begin{minipage}{11.8cm}
		\begin{minted}[frame=single,framesep=2mm,baselinestretch=1.11,fontsize=\small,linenos]{scala}
val awsService = AWSJobService(AWSJobService.Config(
    region = "eu-west-1",
    awsUserName = "adrian",
    awsCredentialsPath = "/Users/adrian/.aws/credentials.csv",
    awsUserId = "434676269080",
    awsKeypairName = "gridscale",
    privateKeyPath = "/Users/adrian/.ssh/id_rsa",
    clusterSize = 1))

awsService.start()

val description = new AWSJobDescription {
  def executable = "/bin/sleep"
  def arguments = "5"
  def workDirectory = aws.home
}

val job = awsService.submit(description)
while (awsService.state(job) != Done) {
  Thread.sleep(WAIT_TIME)
}

awsService.purge(job)
awsService.close()
		\end{minted}
	\end{minipage}
	\caption{Submitting a job to the cloud using the AWS module.}
	\label{AWSJobService}
\end{listing}

\vspace{-5mm}
\subsection{Cluster Deployment} \label{ClusterDeploymentSection}

The design of the module presented above outlines the requirement for a system that can create clusters of AWS EC2 machines and configure them with a job scheduler and a shared filesystem across all nodes.

\textbf{Tool Choice}

During the investigations in Section \ref{ClusterDeploymentSection}, we found that most cost-efficient tools and frameworks for creating the cluster are StarCluster, Elasticluster and Jclouds. The main reason for excluding Mesosphere DC/OS was that it is generally heavyweight and requires running more expensive machines to power the cluster. CfnCluster was dismissed for being too reliant on the AWS software stack, which incurs various extra costs for operations that can easily be performed without Amazon resources.

Jclouds allows mounting various cloud storage devices, but it did not fit our use case since it does not have built-in support for NFS installations. Although Elasticluster can perform all the required tasks, we opted for StarCluster as the cloud deployment tool, since it also provides a load balancer that can be used to optimize the cost or run time of a workflow.

\vspace{3mm}
\textbf{Coordinator Node}
\vspace{1mm}

The choice for StarCluster raises the problem that it is only offered as a command line tool and it needs to be on the machine it is run on. Although most cluster orchestration tools require manual installation and interaction, forcing the user of the library to install an external is both unreasonable and impractical from GridScale's perspective. 

An initial idea could have been running a Docker container with StarCluster, but we can, once again, not assume the presence of a Docker engine on the user's machine.

The solution we picked was to create an EC2 instance with StarCluster already installed. We call this particular instance the cluster \textit{coordinator} or \textit{orchestrator}, since it controls the whole cluster activity by executing the StarCluster commands to operate it. 

The coordinator is launched using Jclouds within the \verb|start| method of the \verb|AWSJobService| created on the local machine. Its configuration is based on a prebuilt AMI with an installation of StarCluster and maintained by the developers of GridScale. After the coordinator is in a running state and its public IP address has been established, the \verb|AWSJobService| can start sending commands to it via an SSH channel. This represents step 1 in Figure \ref{CoordinatorSetup}.

\todo[inline]{Maybe give the URL of this AMI? Might be good to have an official OpenMOLE/GridScale account to store that}


\begin{figure}[h]
	\centering
		\includegraphics[width=0.93\linewidth]{CoordinatorSetup.png}
	\caption{Creating a new cluster using a proxy coordinator node and StarCluster.}
	\label{CoordinatorSetup}
\end{figure}

In order to start the coordinator node, Jclouds needs to authenticate the user. Amazon encourages using IAM\footnote{Identity Access Management} roles for authentication. This means that the user does not need to provide the root account password, but can instead create a user associated with GridScale, which is granted access to manage EC2 instances. Jclouds then uses the access key ID and secret key corresponding to the user role to manage machines via the API. The file containing the keys can be downloaded by the user from the Security Credentials page in the AWS Console \cite{AWSCredentials}. The path to the CSV credentials file must be passed as the \verb|awsCredentialsPath| parameter when creating a new \verb|AWSJobService|.

Apart from using the IAM credentials to make EC2 API calls, we are also opening SSH channels to deliver commands to the coordinator. To do this we need to register the SSH key we are using to initiate the connection to the coordinator on AWS. This is achieved by importing the public key on AWS and giving it a name. The \verb|awsKeypairName| associated with the private key is then always mentioned when openning an SSH connection.

Although it might seem that introducing a proxy node for passing StarCluster commands is inefficient, the actual overhead is minimal. The reason for the limited impact is that StarCluster is only explicitly invoked during the initialisation and destruction of the cluster. All job scheduling interactions with the job submission controller bypass the coordinator, since the job service can obtain the address of the master node and send it jobs directly. The coordinator does indeed incur a cost for being provisionned for as long as the cluster runs, but this is insignificant since we only need one of the cheapest instance types to run it. Further details are discussed in Section \ref{CoordinatorOverheadSection}.

\todo[inline]{I'm not there yet so it might answer my question, but I thought we could shut it down as soon as the cluster is setup?}

\vspace{3mm}
\textbf{StarCluster Configuration}
\vspace{1mm}

StarCluster relies on a configuration file instead of command line parameters to specify the characteristics of clusters it is creating. The file must be located on the machine where StarCluster is being run, so we need to ensure that it is present on the coordinator node. Since the configuration parameters depend on the preferences of the user, the file can not be statically embedded in the AMI that the coordinator is constructed from and needs to be generated dynamically. Listing \ref{StarClusterConfig} shows an example StarCluster configuration file constructed on-the-fly.

\begin{listing}[h]
	\centering
	\begin{minipage}{12cm}
		\begin{minted}[frame=single,framesep=2mm,baselinestretch=1.15,fontsize=\small,linenos]{text}
[global]
DEFAULT_TEMPLATE = jobcluster
ENABLE_EXPERIMENTAL = True

[aws info]
AWS_REGION_NAME = eu-west-1
AWS_REGION_HOST = ec2.eu-west-1.amazonaws.com
AWS_ACCESS_KEY_ID = AKIAJHJE7PZRI4AG26TQ
AWS_SECRET_ACCESS_KEY = 359PuEAFpRgawFrKOBL8KhQEs1uF
AWS_USER_ID = 837461874501

[key starcluster-d4c9b13a]
KEY_LOCATION = .starcluster/starcluster-d4c9b13a

[cluster jobcluster]
KEYNAME = starcluster-d4c9b13a
CLUSTER_SIZE = 1
CLUSTER_USER = sgeadmin
CLUSTER_SHELL = bash
NODE_IMAGE_ID = ami-044abf73
MASTER_INSTANCE_TYPE = m3.medium
NODE_INSTANCE_TYPE = m1.small
		\end{minted}
	\end{minipage}
	\caption{Example StarCluster configuration.}
	\label{StarClusterConfig}
\end{listing}


The authentication method used for bringing up EC2 instances is the same as the one used by Jclouds and StarCluster also needs an SSH keypair in order to set up SSH access to the master node and password-less SSH within the nodes in the cluster. Therefore, as step 2 in Figure \ref{CoordinatorSetup}, we first create a new private key, saved in this case as \verb|starcluster-d4c9b13a|, and import it into AWS.

Step 3 is creating the \verb|config| file on the coordinator node from the parameters passed to the \verb|AWSJobService|. After the job service triggers the cluster initialization at step 4, the coordinator performs the launch using the \verb|starcluster start| command at step 5.

The actions above leave the system in a state where the AWS cluster is initialized and configured with the SGE scheduler, so we can start submitting jobs to the master node, which acts as a submission controller. For the purpose of job management, the job service should now ignore the coordinator and communicate directly with the master of the cluster. Figure \ref{MasterConnection} illustrates how this is achieved.

After obtaining the address of the master, the local machine needs to establish an SSH connection with it. Instead of creating a new set of SSH keys, we chose to reuse the keypair used by the coordinator to communicate with the cluster and transfer the private key locally under a temporary \verb|~/.gridscale| directory. This keypair is already set up for connections to all nodes in the cluster and helps avoid the complexity of managing even more keys.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.96\linewidth]{MasterConnection.png}
	\caption{Connecting the local machine with the master node via SSH and distributing jobs.}
	\label{MasterConnection}
\end{figure}

\vspace{2mm}
\textbf{SGE Delegation}
\vspace{1mm}

StarCluster deploys SGE as its scheduler and, as discussed in Section \ref{DesignMotivationSection}, we are able to delegate all job submission methods of the \verb|AWSJobService| to an \verb|SGEJobService|. This is possible thanks to full compatibility between the interfaces of different GridScale modules. A side effect is that \verb|AWSJobDescription| is just a subtype of \verb|SGEJobDescription|, as they are both tied to their corresponding job services.

\todo[inline]{Why does the JobDescriptions have to be subtyped again?}

 Tasks are specified in the same way as for plain SGE and are eventually translated to SGE jobs. Steps 4 and 5 in Figure \ref{MasterConnection} show the normal job submission mechanism after the setup phase is finished. Once a queue of jobs is populated, the work is distributed by the scheduling system to all nodes in the cluster.

\vspace{1mm}
\textbf{Cluster Lifecycle}

The coordinator node and the cluster are tightly coupled in our design because the cluster can not be altered without the coordinator, which, in turn, is not relevant on its own. Therefore it makes sense for the lifecycle of the two components to be managed together, so the \verb|close| method of \verb|AWSJobService| shuts down the cluster, destroys the orchestrator and kills Jclouds communication channels to AWS.

Even though \verb|start| and \verb|close| could have been implicitly called respectively when an instance of the job service is created and when all jobs are finished, keeping them in the public API allows the user of a library more flexibility towards reusing the service for more sessions without having to repeatedly wait for the initialization steps.

\subsection{Load Balancing}

We use the built-in StarCluster load balancer to decrease or increase the number of nodes in the cluster between limits specified by the user. We start the daemon that performs job monitoring right after launching the cluster and keep the standard 60 seconds polling interval. If the user does not specify a \verb|minimumClusterSize| or \verb|maximumClusterSize| for elastic scaling purposes when creating the job service, then we allow the cluster to shrink or extend in size between 1 node and its initial size. We argue that this is the cautious choice, since it can potentially only reduce costs and not incur any unexpected ones.

Another option we expose is to automatically killing the cluster (by setting the \verb|autoClose| flag) if it is stays completely idle for a certain period after all jobs have been terminated. Some consumers of the library may prefer this as the default option rather than having to explicitly estimate when work is done and call \verb|close|.

To obtain a highly elastic cluster, users can also reduce the 3 minute lookback window via the \verb|lookbackWindow| parameter, which represents the time after which an idle node is considered for elimination. However, note that nodes will never be eliminated unless they have already been running for at least 45 minutes of the hour they have been rented for. Due to Amazon's hourly rental policy, an overly eager resource cutting approach would be non optimal.

\subsection{Spot Instances}

Support for running a cluster consisting entirely of spot instances is leveraging the StarCluster for bidding for the desired type of worker instance. GridScale automates the bid decision by using a simple heuristic based on the current bids for the instance and the average for the past month. We assume that the correct price will always tend towards the average, so we choose a bid that is between the average and the current price. 

If the current price is lower than the average, then we are ahead of the bidders that will soon lose their machines as prices will rise. In the opposite scenario, we know that the equilibrium price is lower than the current one, so we can afford to bid less than the market price while still remaining confident to be competitive.

Overall, the strategy is rather conservative. We believe that no aggressive bids are needed to make a profit, since prices for spot instances are already approximately 6 times lower\footnote{Prices as of June 2016 for instances in US East} than for regular on-demand ones, as seen in Table \ref{SpotPricing}.

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{Price (\$ per hour)}} \\ \cline{2-3} 
\multicolumn{1}{c|}{\textbf{Instance type}} & \multicolumn{1}{c|}{\textbf{Spot}} & \multicolumn{1}{c|}{\textbf{On-Demand}} \\ \hline
\multicolumn{1}{|c|}{c3.large} & \multicolumn{1}{c|}{0.0177} & \multicolumn{1}{c|}{0.105} \\ \hline
\multicolumn{1}{|c|}{m3.medium} & \multicolumn{1}{c|}{0.0108} & \multicolumn{1}{c|}{0.067} \\ \hline
\multicolumn{1}{|c|}{m3.xlarge} & \multicolumn{1}{c|}{0.037} & \multicolumn{1}{c|}{0.266} \\ \hline
\multicolumn{1}{|c|}{m3.2xlarge} & \multicolumn{1}{c|}{0.0842} & \multicolumn{1}{c|}{0.532} \\ \hline
\end{tabular}
\caption{Differences in prices between spot and on-demand instances \cite{AWSPricing}.}
\label{SpotPricing}
\end{table}

\subsection{GCE Version}

\section{OpenMOLE AWS Environment}

\subsection{Resource Mapper}

\todo[inline]{Talk about how the NFS of the job service enables sharing the runtime across execution nodes.}
\todo[inline]{Talk about it deals automatically with job failure since OpenMOLE has a submission mechanism that simply submits the job again if it doesn't get results - so not affected by downsizing.}
\todo[inline]{Mechanism used to detect jobs are done based on the message queue}