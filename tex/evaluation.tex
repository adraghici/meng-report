\chapter{Evaluation} \label{EvaluationChapter}

\todo[inline]{Not worth ever using reserved machines - 12\% cheaper if reserved 3 years in advance}

\section{GridScale Benchmarks}

\todo[inline]{Using instances with really good network connectivity offers diminishing returns, as at some point the latency rather than the bandwidth becomes the dominant factor}

\todo[inline]{Mention not allowing upscaling for the load balancer, because it will just blow up the size since we're just testing the submission / cancelling times}

\section{OpenMOLE Workflow Benchmarks}

\todo[inline]{Also no upscaling because it would fuck up our metrics about how much the experiment costs and how much time a certain size cluster consumes for it}

\todo[inline]{Results influenced by the fact that we couldn't really use the best available machines due to budget. Usually, benchmarks are run on really powerful machines.}

\todo[inline]{Maybe show a table of the instances to indicate how the memory / CPU / prices vary and why we chose those for test}

\todo[inline]{Make the point that it is expected for experiments to be slower on the cloud, since academic grids and local clusters usually benefit from way above average machines or even supercomputers, which are clearly unaffordable in a commercial setting. So being in the same order of magnitude is alright}

\section{Configuration Requirements}

\todo[inline]{Ease of use, especially using resource mapping and being able to keep the memory requirement parameter}

\section{Coordinator Node} \label{CoordinatorOverheadSection}

\todo[inline]{Talk about the performance overhead introduced by the coordinator / T1.MICRO instance}

As described in the project plan, we plan to develop the project incrementally, by having an early functional implementation that we can afterwards iteratively improve. Since we assume a stable, albeit limited, product at each iteration, we will also be able to perform evaluation at each stage. The results of the benchmarks should guide the further development of the project by helping us to identify non-optimal solutions, bugs, or negative user feedback.

The main performance benchmark that we plan to run is against the current speed and resource utilisation of OpenMOLE when submitting jobs to grids and clusters. This will allow us to find the performance problems generated exclusively by the deployment of jobs to the cloud and point to potential bottlenecks. However, we expect performance to slightly decrease, given that standard machines available in cloud environments are less powerful than those designed specifically for high performance computing purposes and used in common scientific grids and clusters \cite{Juve2009}.

We consider direct comparison with other workflow systems not to be a very reliable performance indicator for the new cloud integration, since results would be influenced by the overall structure of the system. We also rate it as a stretch goal because studying the workflow definition procedures for each different workflow systems can be time consuming and most non-generic platforms refer to contextual information in fields that we are not familiar with.

Quantifying the ease of use and quality of the implementation is a more subtle task. Since one of the goals of the project is to achieve automation of deploying clusters and running jobs in the cloud, we can measure the positive impact on the user experience by the degree to which full automation is achieved. Requiring too much user configuration is one of the main drawbacks of competing workflow management and we believe that overcoming this can objectively be considered a success.

OpenMOLE and GridScale are open-source projects, so the implementation quality factor can potentially be evaluated by the level of interference with other components of the modular architecture. Although fragile design and implementation bugs should be observed in the early development stage, we plan to measure the reliability of the new workflow execution environment by running several stress tests against the current execution environments.